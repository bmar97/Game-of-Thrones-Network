{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db8be6-f508-463b-b82d-3abc98ccee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(file_name):\n",
    "    \"\"\"\n",
    "    Function to process text from a text file (.txt) using Spacy.\n",
    "    \n",
    "    Params:\n",
    "    file_name -- name of txt file as string \n",
    "    \n",
    "    Returns:\n",
    "    a processed doc file using Spacy English language model \n",
    "    \n",
    "    \"\"\"\n",
    "    # Load spacy English language model\n",
    "    NER = spacy.load('en_core_web_sm')\n",
    "    NER.max_length = 2400382\n",
    "    book_text = open(book).read()\n",
    "    book_doc = NER(book_text)\n",
    "    return book_doc\n",
    "\n",
    "\n",
    "\n",
    "def get_ne_list_per_sentence(spacy_doc):\n",
    "    \"\"\"\n",
    "    Get a list of entites per sentence of a Spacy document and store in a dataframe.\n",
    "    \n",
    "    Params:\n",
    "    spacy_doc -- a Spacy processed document\n",
    "    \n",
    "    Returns:\n",
    "    a dataframe containing the sentences and corresponding list of recognised named entities in the sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse book by sentence and store each respective named entity in corresponding dictionary 'key: values'\n",
    "    sent_entity_df = []\n",
    "\n",
    "    for sent in book_doc.sents:\n",
    "        entity_list = [ent.text for ent in sent.ents]\n",
    "        sent_entity_df.append({\"sentence\": sent, \"entities\": entity_list})\n",
    "\n",
    "    sent_entity_df = pd.DataFrame(sent_entity_df)\n",
    "\n",
    "    return sent_entity_df\n",
    "\n",
    "\n",
    "def filter_entity(ent_list, char_df):\n",
    "    \"\"\"\n",
    "    Function to filter out non-character entities.\n",
    "    \n",
    "    Params:\n",
    "    ent_list -- list of entities to be filtered\n",
    "    character_df -- a dataframe contain characters' names and characters' first names\n",
    "    \n",
    "    Returns:\n",
    "    a list of entities that are characters (matching by names or first names).\n",
    "    \n",
    "    \"\"\"\n",
    "    return [ent for ent in ent_list\n",
    "            if ent in list(char_df.characters)\n",
    "            or ent in list(char_df.character_firstname)]\n",
    "\n",
    "\n",
    "def create_relationships(df, window_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a dataframe of relationships based on the df dataframe (containing lists of chracters per sentence) and the  window size of n sentences.\n",
    "    \n",
    "    Params:\n",
    "    df -- a dataframe containing a column called character_entities with the list of chracters for each sentence of a document.\n",
    "    window_size -- size of the windows (number of sentences) for creating relationships between two adjacent characters in the text.\n",
    "    \n",
    "    Returns:\n",
    "    a relationship dataframe containing 3 columns: source, target, value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Window size and relationship dictionary instantiation \n",
    "    relationships = []\n",
    "\n",
    "    for i in range(sent_entity_df_filtered.index[-1]):\n",
    "        end_i = min(i+5, sent_entity_df_filtered.index[-1])\n",
    "        char_list = sum((sent_entity_df_filtered.loc[i: end_i].character_entities), [])\n",
    "\n",
    "        # Remove diplicate characters next to each other\n",
    "        char_unique = [char_list[i] for i in range(len(char_list))\n",
    "                      if (i == 0) or char_list[i] != char_list[i-1]] \n",
    "        if len(char_unique) > 1:\n",
    "            for idx, a in enumerate(char_unique[:-1]):\n",
    "                b = char_unique[idx + 1]\n",
    "                relationships.append({\"source\": a, \"target\": b})\n",
    "           \n",
    "    # Transform newly created relationship dictionary into pd DF \n",
    "    relationship_df = pd.DataFrame(relationships)\n",
    "    relationship_df = pd.DataFrame(np.sort(relationship_df.values, axis = 1), columns = relationship_df.columns)\n",
    "\n",
    "    # Find relationship weight of each character and select the first 250 heavily weighted relationships \n",
    "    relationship_df[\"value\"] = 1 \n",
    "    relationship_df = relationship_df.groupby([\"source\",\"target\"], sort=False, as_index=False).sum()\n",
    "    relationship_df = relationship_df.iloc[:233, :]\n",
    "                \n",
    "    return relationship_df\n",
    "\n",
    "\n",
    "def centrality_dev(G):\n",
    "    \n",
    "    # Degree of centrality\n",
    "    degree_dict = nx.degree_centrality(G)\n",
    "    degree_df = pd.DataFrame.from_dict(degree_dict, orient='index', columns=['degree centrality'])\n",
    "\n",
    "    # Betweenness centrality \n",
    "    betweenness_dict = nx.betweenness_centrality(G)\n",
    "    betweenness_df = pd.DataFrame.from_dict(betweenness_dict, orient='index', columns=['betweenness centrality'])\n",
    "    \n",
    "    # Closeness centrality\n",
    "    closeness_dict = nx.closeness_centrality(G)\n",
    "    closeness_df = pd.DataFrame.from_dict(closeness_dict, orient='index', columns=['closeness centrality'])\n",
    "        \n",
    "    return [degree_dict, betweenness_dict, closeness_dict]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
